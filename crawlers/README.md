# Crawlers - Knowledge Acquisition

Automated crawlers for building knowledge graphs from Wikipedia and Wikidata.

---

## ğŸ“Š **Overview**

Two robust crawlers with retry logic, rate limiting, and error handling:

1. **Wikipedia Crawler** - Extracts entities and relationships from Wikipedia pages
2. **Wikidata Crawler** - Queries structured data via SPARQL

**Used in 1000-test benchmark:** Crawled 500+ Wikipedia pages across 10 domains

---

## ğŸ•·ï¸ **Wikipedia Crawler**

### **Features:**
- âœ… Automatic retry (3 attempts, exponential backoff)
- âœ… Rate limiting (2s delay between requests)
- âœ… URL decoding (handles C++, C#, etc.)
- âœ… Entity extraction from page links
- âœ… Category-based classification
- âœ… Relationship discovery

### **Usage:**

```python
from crawlers.wikipedia_crawler import WikipediaCrawler
from core.causal_graph import CausalGraph

# Create graph
graph = CausalGraph()

# Initialize crawler
crawler = WikipediaCrawler(graph, verbose=True)

# Crawl topic
stats = crawler.crawl_topic(
    start_page='Python',
    max_pages=10,
    max_depth=1
)

print(f"Pages crawled: {stats['pages_crawled']}")
print(f"Entities added: {stats['entities_added']}")
print(f"Relations added: {stats['relations_added']}")
```

### **Output:**
```
ğŸ•·ï¸  Wikipedia Crawler
   Topic: Python
   Max Pages: 10, Max Depth: 1

[1/10] Python (depth 0)
[2/10] Python_(programming_language) (depth 1)
[3/10] Guido_van_Rossum (depth 1)
...

âœ… Crawl Complete!
   Pages: 10
   Entities: 108
   Relations: 100
```

### **Parameters:**
- `start_page` (str): Wikipedia page title (URL-decoded automatically)
- `max_pages` (int): Maximum pages to crawl (default: 10)
- `max_depth` (int): Link depth to follow (default: 1)
- `verbose` (bool): Print progress (default: True)

### **Rate Limiting:**
- 2 seconds between requests
- Automatic retry on HTTP 429 (rate limit)
- Exponential backoff (5s, 10s, 20s)

---

## ğŸ” **Wikidata Crawler**

### **Features:**
- âœ… SPARQL queries for structured data
- âœ… Property extraction (P31, P279, etc.)
- âœ… Label resolution (multilingual)
- âœ… Relationship mapping
- âœ… Rate limiting (1.5s delay)

### **Usage:**

```python
from crawlers.wikidata_crawler import WikidataCrawler

crawler = WikidataCrawler(graph, verbose=True)

# Get entity info
entity_id = 'Q28865'  # Python (programming language)
info = crawler.get_entity_info(entity_id)

print(info['label'])  # "Python"
print(info['description'])  # "high-level programming language"
print(info['properties'])  # {'instance_of': 'Q9143', ...}
```

### **Common Properties:**
- `P31`: instance of
- `P279`: subclass of
- `P50`: author
- `P571`: inception
- `P17`: country

---

## ğŸ§ª **Testing**

### **Run All Tests:**
```bash
python test_crawlers.py
```

### **Test Wikipedia Only:**
```python
from crawlers.wikipedia_crawler import test_crawler
test_crawler()
```

### **Test Wikidata Only:**
```python
from crawlers.wikidata_crawler import test_crawler
test_crawler()
```

---

## ğŸ“‹ **Benchmark Usage**

The 1000-test benchmark uses Wikipedia crawler:

```python
# From benchmark_1000_suite.py

DOMAINS = {
    'programming': {
        'topics': ['Python', 'Java', 'JavaScript', ...],
        'max_pages_per_topic': 5
    },
    'science': {
        'topics': ['Gravity', 'Evolution', 'DNA', ...],
        'max_pages_per_topic': 5
    },
    # ... 10 domains total
}

# Crawls: 10 topics Ã— 5 pages = 50 pages per domain
# Total: 500 pages across all domains
# Result: 1,022 entities, 980 relations
```

---

## âš™ï¸ **Configuration**

### **Wikipedia Crawler:**
```python
WikipediaCrawler(
    graph,
    language='en',      # Wikipedia language code
    verbose=True        # Print progress
)

# Internal config:
request_delay = 2.0     # Seconds between requests
max_retries = 3         # Retry attempts
retry_delay = 5.0       # Initial retry delay
```

### **Wikidata Crawler:**
```python
WikidataCrawler(
    graph,
    verbose=True
)

# Internal config:
request_delay = 1.5     # Seconds between requests
max_retries = 3
```

---

## ğŸ› **Error Handling**

### **Common Issues:**

**Problem:** `HTTP 429 - Too Many Requests`  
**Solution:** Automatic retry with exponential backoff

**Problem:** `Page not found`  
**Solution:** Logged as failed, continues with next page

**Problem:** `Connection timeout`  
**Solution:** 3 retries with 5s delay

**Problem:** `URL encoding (C++, C#)`  
**Solution:** Automatic URL decoding with `urllib.parse.unquote`

---

## ğŸ“Š **Statistics**

### **1000-Test Benchmark Crawl Stats:**

```json
{
  "pages_crawled": 99,
  "entities_added": 1022,
  "relations_added": 980
}
```

### **Per-Domain Stats:**

| Domain | Pages | Entities | Relations |
|--------|-------|----------|-----------|
| Programming | 10 | 108 | 100 |
| Science | 10 | 109 | 100 |
| Technology | 10 | 108 | 100 |
| History | 10 | 103 | 100 |
| Geography | 10 | 107 | 100 |
| Biology | 10 | 88 | 100 |
| Physics | 9 | 82 | 80 |
| Literature | 10 | 110 | 100 |
| Music | 10 | 104 | 100 |
| Art | 10 | 103 | 100 |

---

## ğŸ”§ **Advanced Usage**

### **Custom Entity Extraction:**

```python
class CustomCrawler(WikipediaCrawler):
    def _extract_custom_data(self, page_data):
        # Add custom extraction logic
        pass
```

### **Filter by Category:**

```python
# In _crawl_page(), filter categories:
if 'Programming languages' in category_names:
    # Process differently
    pass
```

### **Custom SPARQL Queries:**

```python
query = """
SELECT ?item ?itemLabel WHERE {
  ?item wdt:P31 wd:Q9143.  # instance of programming language
  SERVICE wikibase:label { bd:serviceParam wikibase:language "en". }
}
"""
crawler.sparql_query(query)
```

---

## ğŸ“ **File Structure**

```
crawlers/
â”œâ”€â”€ wikipedia_crawler.py    # Wikipedia API crawler
â”œâ”€â”€ wikidata_crawler.py     # Wikidata SPARQL crawler
â”œâ”€â”€ test_crawlers.py        # Validation tests
â””â”€â”€ README.md               # This file
```

---

## ğŸ¯ **Key Features**

âœ… **Robust:** Retry logic, error handling  
âœ… **Respectful:** Rate limiting, user agent  
âœ… **Flexible:** Configurable depth, page limits  
âœ… **Tested:** 1000-test benchmark validation  
âœ… **Documented:** Complete API documentation  

---

## ğŸ“§ **Contact**

Issues? Email: info@marchesse.de
